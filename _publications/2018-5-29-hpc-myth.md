---
title: "Hardware performance counters can detect malware: Myth or fact?"
collection: publications
permalink: /publication/2018-5-29-hpc-myth
excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
date: 2018-5-29
venue: 'AsiaCSS 2018'
paperurl: 'https://dl.acm.org/doi/pdf/10.1145/3196494.3196515'
citation: '@inproceedings{10.1145/3196494.3196515, author = {Zhou, Boyou and Gupta, Anmol and Jahanshahi, Rasoul and Egele, Manuel and Joshi, Ajay}, title = {Hardware Performance Counters Can Detect Malware: Myth or Fact?}, year = {2018}, isbn = {9781450355766}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3196494.3196515}, doi = {10.1145/3196494.3196515}, pages = {457â€“468}, numpages = {12}, keywords = {malware detection, machine learning, hardware performance counters}, location = {Incheon, Republic of Korea}, series = {ASIACCS '18} }'
---
The ever-increasing prevalence of malware has led to the explo- rations of various detection mechanisms. Several recent works propose to use Hardware Performance Counters (HPCs) values with machine learning classiication models for malware detection. HPCs are hardware units that record low-level micro-architectural behavior, such as cache hits/misses, branch (mis)prediction, and load/store operations. However, this information does not reliably capture the nature of the application, i.e. whether it is benign or malicious. In this paper, we claim and experimentally support that using the micro-architectural level information obtained from HPCs cannot distinguish between benignware and malware. We eval- uate the idelity of malware detection using HPCs. We perform quantitative analysis using Principal Component Analysis (PCA) to systematically select micro-architectural events that have the most predictive powers. We then run 1,924 programs, 962 benignware and 962 malware, on our experimental setups. We achieve 83.39%, 84.84%, 83.59%, 75.01%, 78.75%, and 14.32% F1-score (a metric of detection rates) of Decision Tree (DT), Random Forest (RF), K Near- est Neighbors (KNN), Adaboost, Neural Net (NN), and Naive Bayes, respectively. We cross-validate our models 1,000 times to show the distributions of detection rates in various models. Our cross- validation analysis shows that many of the experiments produce low F1-scores. The F1-score of models in DT, RF, KNN, Adaboost, NN, and Naive Bayes is 80.22%, 81.29%, 80.22%, 70.32%, 35.66%, and 9.903%, respectively. To further highlight the incapability of malware detection using HPCs, we show that one benignware (Notepad++) infused with malware (ransomware) cannot be de- tected by HPC-based malware detection.

[Download paper here](https://dl.acm.org/doi/pdf/10.1145/3196494.3196515)
